## 实验一：作业调度算法
### 错误原因
HRRN的排序逻辑是没有错的，错的是“时间”的计算——我们把小时制错当做普通分钟/秒数来计算了。
这会导致进程6和7晚进入等待队列，从而导致响应比排序错误。

我们要把startTime、finishTime中普通的分钟数在60进位和100进位之间转换。比如：880 -> 920,895 -> 935。  
（老师只看输出的顺序是否正确，因此我们在arrivalTime传入时，将60进制转换为100进制即可）

### 实验笔记
**执行顺序：**  
  
FIFO：1->2->3->4->5->6->7  
  
SJF（最短优先）：1->5->6->4->7->3->2  
  
HRRN（最高响应比优先）：1->2->5->6->7->4->3  
  
响应比计算：1 + 作业等待时间 / 作业执行时间 = 1 + (完成时间 - 到达时间 - 执行时间) / 作业执行时间  
  
（作业等待时间：进程在内存中（从到达到执行结束），但是不拉屎（未在执行中）的时间）  
  
输入：
```
1 800 50
2 815 30
3 830 25
4 835 20
5 845 15
6 900 10
7 920 5
-1
```
时间单位统一后：
```
1 800 50
2 815 30
3 830 25
4 835 20
5 845 15
6 860 10
7 880 5
-1
```

## 实验二：进程调度算法
### 实验笔记
输入：
process.txt中，**输入数据各数据项**的意义（从左向右）：  
| 进程id | 进程状态（是否就绪） | 到达时间 | 执行时间 | 优先级（从3~0，**优先级从大变小**） |

时间片为3（示例）。  

输出：
第一行：进程执行顺序  
第三~六行：每个进程的执行时间、等待时间、周转时间  
第七行：平均等待时间  
第八行：平均周转时间  


## 实验三：分区式存储管理算法
### 算法原理   
1. 首次适应算法：找到的 **第一个** 大于等于 进程占用空间的内存块，就返回该内存块作为结果。  
  
2. 最佳适应算法：从头到尾遍历一整个内存，找到 **足够大** 的内存块中 **最小** 的那个内存块，返回作为结果。  
  
3. 最差适应算法：和最佳适应差不多，只不过返回的是 **剩余** 内存块中 **最大**的一个。

### 实验笔记  
输入：  
Partitions：{ 起始地址，内存大小 }（——凳子的位置&大小）  
requests：请求（进程）大小（——人屁股的大小）  

输出：
1. 某算法寻找合适内存块时的过程：  
- "Trying to allocate { 当前进程大小 } units of memory"   
- "Checking block { 当前被检查的内存块序号 } with size { 内存块的剩余空闲内存大小 }"  
......  
- "Found ... block { 最合适的内存块序号 }, which start with { 选中内存块的**可占用内存的起始地址** }. Allocating { 当前进程大小 } units."  
   
  
2. 算法中所有进程的汇总结果：   
"Process xx"：显示不同进程**最后占用的内存块**的**地址和大小**  
"Remaining Partiions"：**剩下的空内存块**的**起始地址和大小**  
  

## 实验四：页面调度算法
### 实验笔记   
输入：   
```
1 2 3 4 1 2 5 1 2 3 4 5
（总共是读取12次进程）
frameCount = 3
```
frameCount：内存队列的大小（当当前要读取的进程不在内存队列中，且内存队列已满时，就要根据不同标准，从内存队列中踢出一个进程）   

输出：  
Evicted pages：发生缺页中断时，被排出内存队列的进程序号。  
Total page faults：总缺页次数, Page fault rate：缺页率  